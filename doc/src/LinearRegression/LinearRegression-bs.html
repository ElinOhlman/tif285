<!--
Automatically generated HTML file from DocOnce source
(https://github.com/hplgit/doconce/)
-->
<html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="DocOnce: https://github.com/hplgit/doconce/" />
<meta name="viewport" content="width=device-width, initial-scale=1.0" />
<meta name="description" content="Learning from data: Linear Regression">

<title>Learning from data: Linear Regression</title>

<!-- Bootstrap style: bootstrap -->
<link href="https://netdna.bootstrapcdn.com/bootstrap/3.1.1/css/bootstrap.min.css" rel="stylesheet">
<!-- not necessary
<link href="https://netdna.bootstrapcdn.com/font-awesome/4.0.3/css/font-awesome.css" rel="stylesheet">
-->

<style type="text/css">

/* Add scrollbar to dropdown menus in bootstrap navigation bar */
.dropdown-menu {
   height: auto;
   max-height: 400px;
   overflow-x: hidden;
}

/* Adds an invisible element before each target to offset for the navigation
   bar */
.anchor::before {
  content:"";
  display:block;
  height:50px;      /* fixed header height for style bootstrap */
  margin:-50px 0 0; /* negative fixed header height */
}
</style>


</head>

<!-- tocinfo
{'highest level': 1,
 'sections': [('Linear regression', 1, None, '___sec0'),
              ('Why Linear Regression (aka Ordinary Least Squares)',
               2,
               None,
               '___sec1'),
              ('Regression analysis, overarching aims', 3, None, '___sec2'),
              ('Example: Liquid-drop model for nuclear binding energies',
               3,
               None,
               '___sec3'),
              ('Polynomial basis functions', 2, None, '___sec4'),
              ('General basis functions', 3, None, '___sec5'),
              ('Training scores', 2, None, '___sec6'),
              ('The $\\chi^2$ function', 2, None, '___sec7')]}
end of tocinfo -->

<body>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: {
     equationNumbers: {  autoNumber: "AMS"  },
     extensions: ["AMSmath.js", "AMSsymbols.js", "autobold.js", "color.js"]
  }
});
</script>
<script type="text/javascript" async
 src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>



    
<!-- Bootstrap navigation bar -->
<div class="navbar navbar-default navbar-fixed-top">
  <div class="navbar-header">
    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target=".navbar-responsive-collapse">
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
      <span class="icon-bar"></span>
    </button>
    <a class="navbar-brand" href="LinearRegression-bs.html">Learning from data: Linear Regression</a>
  </div>

  <div class="navbar-collapse collapse navbar-responsive-collapse">
    <ul class="nav navbar-nav navbar-right">
      <li class="dropdown">
        <a href="#" class="dropdown-toggle" data-toggle="dropdown">Contents <b class="caret"></b></a>
        <ul class="dropdown-menu">
     <!-- navigation toc: --> <li><a href="#___sec0" style="font-size: 80%;"><b>Linear regression</b></a></li>
     <!-- navigation toc: --> <li><a href="#___sec1" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Why Linear Regression (aka Ordinary Least Squares)</a></li>
     <!-- navigation toc: --> <li><a href="#___sec2" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Regression analysis, overarching aims</a></li>
     <!-- navigation toc: --> <li><a href="#___sec3" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Example: Liquid-drop model for nuclear binding energies</a></li>
     <!-- navigation toc: --> <li><a href="#___sec4" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Polynomial basis functions</a></li>
     <!-- navigation toc: --> <li><a href="#___sec5" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;General basis functions</a></li>
     <!-- navigation toc: --> <li><a href="#___sec6" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;Training scores</a></li>
     <!-- navigation toc: --> <li><a href="#___sec7" style="font-size: 80%;">&nbsp;&nbsp;&nbsp;The \( \chi^2 \) function</a></li>

        </ul>
      </li>
    </ul>
  </div>
</div>
</div> <!-- end of navigation bar -->

<div class="container">

<p>&nbsp;</p><p>&nbsp;</p><p>&nbsp;</p> <!-- add vertical space -->

<!-- ------------------- main content ---------------------- -->



<div class="jumbotron">
<center><h1>Learning from data: Linear Regression</h1></center>  <!-- document title -->

<p>
<!-- author(s): Christian Forss&#233;n, and Morten Hjorth-Jensen -->

<center>
<b>Christian Forss&#233;n</b> [1]
</center>

<center>
<b>Morten Hjorth-Jensen</b> [2, 3]
</center>

<p>
<!-- institution(s) -->

<center>[1] <b>Department of Physics, Chalmers University of Technology, Sweden</b></center>
<center>[2] <b>Department of Physics, University of Oslo</b></center>
<center>[3] <b>Department of Physics and Astronomy and National Superconducting Cyclotron Laboratory, Michigan State University</b></center>
<br>
<p>
<center><h4>Sep 1, 2020</h4></center> <!-- date -->
<br>
<p>
<!-- potential-jumbotron-button -->
</div> <!-- end jumbotron -->

<!-- !split -->

<h1 id="___sec0" class="anchor">Linear regression </h1>

<p>
<!-- !split -->

<h2 id="___sec1" class="anchor">Why Linear Regression (aka Ordinary Least Squares) </h2>

<p>
Fitting a continuous function with linear parameterization in terms of the parameters  \( \boldsymbol{\theta} \).

<ul>
<li> Often used for fitting a continuous function!</li>
<li> Gives an excellent introduction to central Machine Learning features with <b>understandable pedagogical</b> links to other methods like <b>Neural Networks</b>, <b>Support Vector Machines</b> etc</li>
<li> Analytical expression for the fitting parameters \( \boldsymbol{\theta} \)</li>
<li> Analytical expressions for statistical propertiers like mean values, variances, confidence intervals and more</li>
<li> Analytical relation with probabilistic interpretations</li> 
<li> Easy to introduce basic concepts like bias-variance tradeoff, cross-validation, resampling and regularization techniques and many other ML topics</li>
<li> Easy to code! And links well with classification problems and logistic regression and neural networks</li>
<li> Allows for <b>easy</b> hands-on understanding of gradient descent methods</li>
</ul>

<!-- !split -->

<h3 id="___sec2" class="anchor">Regression analysis, overarching aims  </h3>

<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->

<p>
Regression modeling deals with the description of a <b>response</b> variable(s) \( y \) and how it varies as function of some <b>predictor</b> variable(s) \( x \). The first variable is also often called the <b>dependent</b>, or the <b>outcome</b> variable while the second one can be called the <b>independent</b> variable, or the <b>explanatory</b> variable. Note also that each of these might be a vector of variables, meaning that there could be more than one response variable and more than one predictor variable.

<p>
In general we will try to find a model \( M \) that corresponds to a function \( f_\theta(x) \) such that 
$$
y \approx f_\theta(x)
$$

In <b>linear regression</b> the dependence on the model parameters is <b>linear</b>, and this fact will make it possible to find an analytical expression for the optimal set of model parameters (as we will see below).
</div>
</div>


<p>
<!-- !split -->

<p>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
When performing a regression analysis we will have access to a set of data \( \mathcal{D} \) that consists of:

<ul>
<li> \( n \) cases \( i = 0, 1, 2, \dots, n-1 \)</li> 
</ul>

For each case there is a

<ul>
<li> (vector of) response variable(s) \( y_i \) (observations);</li>
<li> (vector of) independent variable(s) \( x_i \).</li>
</ul>

Below, we will use boldface to denote the set of data, i.e., \( \boldsymbol{y} = (y_0, y_1,\ldots, y_{n-1}) \) and \( \boldsymbol{x} = (x_0, x_1,\ldots, x_{n-1}) \).

<p>
The independent variables can be turned into a number of <b>features</b>, and the key to a successful regression analysis is to identify the most relevant features. In physics, these would correspond to a set of <b>basis functions</b>.

<p>
Assume that there are \( p \) features and we will use the (possibly confusing) notation

<ul>
<li> \( x_i=[x_{i0}, x_{i1}, \dots, x_{ip-1}] \) and from now on let \( x \) denote the vector of features. See below for more explicit examples.</li> 
</ul>

As our model will (in general) not predict the observations perfectly, we will write the relationship as
$$
y_i = f_\theta(x_i) + \epsilon_i,
$$

where \( \epsilon_i \) is the error (or the <b>residual</b>).
</div>
</div>


<p>
<!-- !split -->

<p>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
A regression analysis aims at finding the model parameters \( \theta \) of a specified model \( M \) such that the vector of errors \( \boldsymbol{\epsilon} \) is minimized. You might ask the very relevant question what is specifically meant by minimizing a vector, and you will find that this is often achieved by minimizing a <b>cost</b> function that has been introduced without much motivation. This function might also be called a <b>loss</b> function or an <b>objective</b> function.

<p>
Alternatively, we could introduce the likelihood function \( p(\boldsymbol{y} \vert \boldsymbol{x}, M(\theta)) \).  It is the conditional distribution for the probability of making the observations \( \boldsymbol{y} \) given the independent variable \( \boldsymbol{x} \) and a model \( M \), where \( \boldsymbol{y} \) and \( \boldsymbol{x} \) are contained in our data set \( \mathcal{D} \). The parameters \( \theta \) that maximizes this likelihood function is then our optimal set. We will later discuss likelihood functions in much more detail.

<p>
Having access to this &quot;optimal&quot; model, we have extracted a relationship between \( \boldsymbol{y} \) and \( \boldsymbol{x} \) that we can exploit to infer causal dependencies, make predictions, and many other things.
</div>
</div>


<p>
<!-- !split -->
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
The \( p \) explanatory variables for the \( n \) cases in the data set are normally represented by a matrix \( \mathbf{X} \). The matrix \( \mathbf{X} \) is called the <em>design
matrix</em>.
</div>
</div>


<p>
<!-- !split -->

<h3 id="___sec3" class="anchor">Example: Liquid-drop model for nuclear binding energies  </h3>

<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
In order to understand the relation among the predictors \( p \), the set of data \( \mathcal{D}_n \) and the target (outcome, output etc) \( \boldsymbol{y} \),
consider the model we discussed for describing nuclear binding energies.

<p>
There we assumed that we could parametrize the data using a polynomial approximation based on the liquid drop model.
Assuming 
$$
BE(A,N,Z) = a_0+a_1A+a_2A^{2/3}+a_3 Z^2 A^{-1/3}+a_4 (N-Z)^2 A^{-1},
$$

we have five features, that is the intercept (constant term, aka bias), the \( A \) dependent term, the \( A^{2/3} \) term and the \( Z^2 A^{-1/3} \) and \( (N-Z)^2 A^{-1} \) terms. Although the features are somewhat complicated functions of the independent variables \( A,N,Z \), we note that the \( p=5 \) regression parameters \( \theta = (a_0, a_1, a_2, a_3, a_4) \) enter linearly. Furthermore we have \( n \) cases. It means that our design matrix is a 
\( p\times n \) matrix \( \boldsymbol{X} \).

<p>
</div>
</div>


<p>
<!-- !split -->

<h2 id="___sec4" class="anchor">Polynomial basis functions </h2>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
The perhaps simplest linear-regression approach is to assume we can parametrize our function in terms of a polynomial \( f(x) \) of degree \( p-1 \). I.e.
$$
y(x_i)=f({x}_i)+\epsilon_i=\sum_{j=0}^{p-1} \theta_j x_i^j+\epsilon_i,
$$

where \( \epsilon_i \) is the error in our approximation.

<p>
</div>
</div>


<p>
<!-- !split -->
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
For every set of values \( y_i,x_i \) we have thus the corresponding set of equations
$$
\begin{align*}
y_0&=\theta_0+\theta_1x_0^1+\theta_2x_0^2+\dots+\theta_{p-1}x_0^{p-1}+\epsilon_0\\
y_1&=\theta_0+\theta_1x_1^1+\theta_2x_1^2+\dots+\theta_{p-1}x_1^{p-1}+\epsilon_1\\
y_2&=\theta_0+\theta_1x_2^1+\theta_2x_2^2+\dots+\theta_{p-1}x_2^{p-1}+\epsilon_2\\
\dots & \dots \\
y_{n-1}&=\theta_0+\theta_1x_{n-1}^1+\theta_2x_{n-1}^2+\dots+\theta_{p-1}x_{n-1}^{p-1}+\epsilon_{n-1}.\\
\end{align*}
$$
</div>
</div>


<p>
<!-- !split -->
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
Defining the vectors
$$
\boldsymbol{y} = [y_0,y_1, y_2,\dots, y_{n-1}]^T,
$$

and
$$
\boldsymbol{\theta} = [\theta_0,\theta_1, \theta_2,\dots, \theta_{p-1}]^T,
$$

and
$$
\boldsymbol{\epsilon} = [\epsilon_0,\epsilon_1, \epsilon_2,\dots, \epsilon_{n-1}]^T,
$$

and the design matrix
$$
\boldsymbol{X}=
\begin{bmatrix} 
1& x_{0}^1 &x_{0}^2& \dots & \dots &x_{0}^{p-1}\\
1& x_{1}^1 &x_{1}^2& \dots & \dots &x_{1}^{p-1}\\
1& x_{2}^1 &x_{2}^2& \dots & \dots &x_{2}^{p-1}\\                      
\dots& \dots &\dots& \dots & \dots &\dots\\
1& x_{n-1}^1 &x_{n-1}^2& \dots & \dots &x_{n-1}^{p-1}\\
\end{bmatrix} 
$$

we can rewrite our equations as
$$
\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\theta}+\boldsymbol{\epsilon}.
$$

The above design matrix is called a <a href="https://en.wikipedia.org/wiki/Vandermonde_matrix" target="_self">Vandermonde matrix</a>.
</div>
</div>


<p>
<!-- !split -->

<h3 id="___sec5" class="anchor">General basis functions  </h3>

<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->

<p>
We are obviously not limited to the above polynomial expansions.  We
could replace the various powers of \( x \) with elements of Fourier
series or instead of \( x_i^j \) we could have \( \cos{(j x_i)} \) or \( \sin{(j
x_i)} \), or time series or other orthogonal functions.  For every set
of values \( y_i,x_i \) we can then generalize the equations to

$$
\begin{align*}
y_0&=\theta_0x_{00}+\theta_1x_{01}+\theta_2x_{02}+\dots+\theta_{p-1}x_{0p-1}+\epsilon_0\\
y_1&=\theta_0x_{10}+\theta_1x_{11}+\theta_2x_{12}+\dots+\theta_{p-1}x_{1p-1}+\epsilon_1\\
y_2&=\theta_0x_{20}+\theta_1x_{21}+\theta_2x_{22}+\dots+\theta_{p-1}x_{2p-1}+\epsilon_2\\
\dots & \dots \\
y_{i}&=\theta_0x_{i0}+\theta_1x_{i1}+\theta_2x_{i2}+\dots+\theta_{p-1}x_{ip-1}+\epsilon_i\\
\dots & \dots \\
y_{n-1}&=\theta_0x_{n-1,0}+\theta_1x_{n-1,2}+\theta_2x_{n-1,2}+\dots+\theta_{p-1}x_{n-1,p-1}+\epsilon_{n-1}.\\
\end{align*}
$$

<p>
</div>
</div>


<p>
<!-- !split -->
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
We redefine in turn the matrix \( \boldsymbol{X} \) as
$$
\boldsymbol{X}=
\begin{bmatrix} 
x_{00}& x_{01} &x_{02}& \dots & \dots &x_{0,p-1}\\
x_{10}& x_{11} &x_{12}& \dots & \dots &x_{1,p-1}\\
x_{20}& x_{21} &x_{22}& \dots & \dots &x_{2,p-1}\\                      
\dots& \dots &\dots& \dots & \dots &\dots\\
x_{n-1,0}& x_{n-1,1} &x_{n-1,2}& \dots & \dots &x_{n-1,p-1}\\
\end{bmatrix} 
$$

and without loss of generality we rewrite again our equations as
$$
\boldsymbol{y} = \boldsymbol{X}\boldsymbol{\theta}+\boldsymbol{\epsilon}.
$$

The left-hand side of this equation is kwown. The error vector \( \boldsymbol{\epsilon} \) and the parameter vector \( \boldsymbol{\theta} \) are unknown quantities. How can we obtain the optimal set of \( \theta_i \) values?
</div>
</div>


<p>
<!-- !split -->
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
We have defined the matrix \( \boldsymbol{X} \) via the equations
$$
\begin{align*}
y_0&=\theta_0x_{00}+\theta_1x_{01}+\theta_2x_{02}+\dots+\theta_{p-1}x_{0p-1}+\epsilon_0\\
y_1&=\theta_0x_{10}+\theta_1x_{11}+\theta_2x_{12}+\dots+\theta_{p-1}x_{1p-1}+\epsilon_1\\
y_2&=\theta_0x_{20}+\theta_1x_{21}+\theta_2x_{22}+\dots+\theta_{p-1}x_{2p-1}+\epsilon_1\\
\dots & \dots \\
y_{i}&=\theta_0x_{i0}+\theta_1x_{i1}+\theta_2x_{i2}+\dots+\theta_{p-1}x_{ip-1}+\epsilon_1\\
\dots & \dots \\
y_{n-1}&=\theta_0x_{n-1,0}+\theta_1x_{n-1,2}+\theta_2x_{n-1,2}+\dots+\theta_{p-1}x_{n-1,p-1}+\epsilon_{n-1}.\\
\end{align*}
$$

<p>
Note that the design matrix 
 \( \boldsymbol{X}\in {\mathbb{R}}^{n\times p} \), with the predictors refering to the column numbers and the entries \( n \) being the row elements.
</div>
</div>


<p>
<!-- !split -->
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
With the above we use the design matrix to define the approximation \( \boldsymbol{\tilde{y}} \) via the unknown quantity \( \boldsymbol{\theta} \) as
$$
\boldsymbol{\tilde{y}}= \boldsymbol{X}\boldsymbol{\theta},
$$

and in order to find the optimal parameters \( \theta_i \) instead of solving the above linear algebra problem, we define a function which gives a measure of the spread between the values \( y_i \) (which represent hopefully the exact values) and the parameterized values \( \tilde{y}_i \), namely
$$
C(\boldsymbol{\theta})=\frac{1}{n}\sum_{i=0}^{n-1}\left(y_i-\tilde{y}_i\right)^2=\frac{1}{n}\left\{\left(\boldsymbol{y}-\boldsymbol{\tilde{y}}\right)^T\left(\boldsymbol{y}-\boldsymbol{\tilde{y}}\right)\right\},
$$

or using the matrix \( \boldsymbol{X} \) and in a more compact matrix-vector notation as
$$
C(\boldsymbol{\theta})=\frac{1}{n}\left\{\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)^T\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)\right\}.
$$

This function is one possible way to define the so-called <b>cost function</b>.

<p>
It is also common to define
the cost function as

$$
C(\boldsymbol{\theta})=\frac{1}{2n}\sum_{i=0}^{n-1}\left(y_i-\tilde{y}_i\right)^2,
$$

since when taking the first derivative with respect to the unknown parameters \( \theta \), the factor of \( 2 \) cancels out.
</div>
</div>


<p>
<!-- !split -->
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->

<p>
The function 
$$
C(\boldsymbol{\theta})=\frac{1}{n}\left\{\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)^T\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)\right\},
$$

can be linked to the variance of the quantity \( y_i \) if we interpret the latter as the mean value. 
When linking (see the discussion below) with the maximum likelihood approach, we will indeed interpret \( y_i \) as a mean value
$$
y_{i}=\langle y_i \rangle = \theta_0x_{i,0}+\theta_1x_{i,1}+\theta_2x_{i,2}+\dots+\theta_{n-1}x_{i,n-1}+\epsilon_i,
$$

<p>
where \( \langle y_i \rangle \) is the mean value. Keep in mind also that
till now we have treated \( y_i \) as the exact value. Normally, the
response (dependent or outcome) variable \( y_i \) the outcome of a
numerical experiment or another type of experiment and is thus only an
approximation to the true value. It is then always accompanied by an
error estimate, often limited to a statistical error estimate. For now, we
will treat \( y_i \) as our exact value for the response variable.

<p>
In order to find the parameters \( \theta_i \) we will then minimize the spread of \( C(\boldsymbol{\theta}) \), that is we are going to solve the problem
$$
{\displaystyle \min_{\boldsymbol{\theta}\in
{\mathbb{R}}^{p}}}\frac{1}{n}\left\{\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)^T\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right)\right\}.
$$

In practical terms it means we will require
$$
\frac{\partial C(\boldsymbol{\theta})}{\partial \theta_j} = \frac{\partial }{\partial \theta_j}\left[ \frac{1}{n}\sum_{i=0}^{n-1}\left(y_i-\theta_0x_{i,0}-\theta_1x_{i,1}-\theta_2x_{i,2}-\dots-\theta_{n-1}x_{i,n-1}\right)^2\right]=0, 
$$

which results in
$$
\frac{\partial C(\boldsymbol{\theta})}{\partial \theta_j} = -\frac{2}{n}\left[ \sum_{i=0}^{n-1}x_{ij}\left(y_i-\theta_0x_{i,0}-\theta_1x_{i,1}-\theta_2x_{i,2}-\dots-\theta_{n-1}x_{i,n-1}\right)\right]=0, 
$$

or in a matrix-vector form as
$$
\frac{\partial C(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} = 0 = \boldsymbol{X}^T\left( \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right).  
$$

<p>
</div>
</div>


<p>
<!-- !split -->
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
We can rewrite
$$
\frac{\partial C(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} = 0 = \boldsymbol{X}^T\left( \boldsymbol{y}-\boldsymbol{X}\boldsymbol{\theta}\right),  
$$

as
$$
\boldsymbol{X}^T\boldsymbol{y} = \boldsymbol{X}^T\boldsymbol{X}\boldsymbol{\theta},  
$$

and if the matrix \( \boldsymbol{X}^T\boldsymbol{X} \) is invertible we have the solution
$$
\boldsymbol{\theta} =\left(\boldsymbol{X}^T\boldsymbol{X}\right)^{-1}\boldsymbol{X}^T\boldsymbol{y}.
$$

<p>
We note also that since our design matrix is defined as \( \boldsymbol{X}\in
{\mathbb{R}}^{n\times p} \), the product \( \boldsymbol{X}^T\boldsymbol{X} \in
{\mathbb{R}}^{p\times p} \).  In the liquid drop model example from the Intro lecture, we had \( p=5 \) (\( p \ll n \)) meaning that we end up with inverting a small
\( 5\times 5 \) matrix. This is a rather common situation, in many cases we end up with low-dimensional
matrices to invert, which
allow for the usage of direct linear algebra methods such as <b>LU</b> decomposition or <b>Singular Value Decomposition</b> (SVD) for finding the inverse of the matrix
\( \boldsymbol{X}^T\boldsymbol{X} \).
</div>
</div>


<p>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->
<b>Small question</b>: What kind of problems can we expect when inverting the matrix  \( \boldsymbol{X}^T\boldsymbol{X} \)?
</div>
</div>


<p>
<!-- !split -->

<h2 id="___sec6" class="anchor">Training scores </h2>

<p>
We can easily test our fit by computing various <b>training scores</b>. Several such measures are used in machine learning applications. First we have the <b>Mean-Squared Error</b> (MSE)
$$
\mathrm{MSE}(\boldsymbol{\theta}) = \frac{1}{n} \sum_{i=1}^n \left( y_{\mathrm{data},i} - y_{\mathrm{model},i}(\boldsymbol{\theta}) \right)^2,
$$

where we have \( n \) training data and our model is a function of the parameter vector \( \boldsymbol{\theta} \).

<p>
Furthermore, we have the <b>mean absolute error</b> (MAE) defined as.
$$
\mathrm{MAE}(\boldsymbol{\theta}) = \frac{1}{n} \sum_{i=1}^n \left| y_{\mathrm{data},i} - y_{\mathrm{model},i}(\boldsymbol{\theta}) \right|,
$$

<p>
And the \( R2 \) score, also known as <em>coefficient of determination</em> is
$$
\mathrm{R2}(\boldsymbol{\theta}) = 1 - \frac{\sum_{i=1}^n \left( y_{\mathrm{data},i} - y_{\mathrm{model},i}(\boldsymbol{\theta}) \right)^2}{\sum_{i=1}^n \left( y_{\mathrm{data},i} - \bar{y}_\mathrm{model}(\boldsymbol{\theta}) \right)^2},
$$

where \( \bar{y}_\mathrm{model}(\boldsymbol{\theta}) = \frac{1}{n} \sum_{i=1}^n y_{\mathrm{model},i} (\boldsymbol{\theta}) \) is the mean of the model predictions.

<p>
<!-- !split -->

<h2 id="___sec7" class="anchor">The \( \chi^2 \) function  </h2>
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->

<p>
Normally, the response (dependent or outcome) variable \( y_i \) is the
outcome of a numerical experiment or another type of experiment and is
thus only an approximation to the true value. It is then always
accompanied by an error estimate, often limited to a statistical error
estimate given by a <b>standard deviation</b>.

<p>
Introducing the standard deviation \( \sigma_i \) for each measurement
\( y_i \) (assuming uncorrelated errors), we define the so called \( \chi^2 \) function as

$$
\chi^2(\boldsymbol{\theta})=\frac{1}{n}\sum_{i=0}^{n-1}\frac{\left(y_i-\tilde{y}_i\right)^2}{\sigma_i^2}=\frac{1}{n}\left\{\left(\boldsymbol{y}-\boldsymbol{\tilde{y}}\right)^T \boldsymbol{\Sigma}^{-1}\left(\boldsymbol{y}-\boldsymbol{\tilde{y}}\right)\right\},
$$

where the matrix \( \boldsymbol{\Sigma} \) is a diagonal \( n \times n \) matrix with \( \sigma_i^2 \) as matrix elements.

<p>
</div>
</div>


<p>
<!-- !split -->
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->

<p>
In order to find the parameters \( \theta_i \) we will then minimize the \( \chi^2(\boldsymbol{\theta}) \) function by requiring
$$
\frac{\partial \chi^2(\boldsymbol{\theta})}{\partial \theta_j} = \frac{\partial }{\partial \theta_j}\left[ \frac{1}{n}\sum_{i=0}^{n-1}\left(\frac{y_i-\theta_0x_{i,0}-\theta_1x_{i,1}-\theta_2x_{i,2}-\dots-\theta_{n-1}x_{i,n-1}}{\sigma_i}\right)^2\right]=0, 
$$

which results in
$$
\frac{\partial \chi^2(\boldsymbol{\theta})}{\partial \theta_j} = -\frac{2}{n}\left[ \sum_{i=0}^{n-1}\frac{x_{ij}}{\sigma_i}\left(\frac{y_i-\theta_0x_{i,0}-\theta_1x_{i,1}-\theta_2x_{i,2}-\dots-\theta_{n-1}x_{i,n-1}}{\sigma_i}\right)\right]=0, 
$$

or in a matrix-vector form as
$$
\frac{\partial \chi^2(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} = 0 = \boldsymbol{A}^T\left( \boldsymbol{b}-\boldsymbol{A}\boldsymbol{\theta}\right).  
$$

where we have defined the matrix \( \boldsymbol{A} =\boldsymbol{X} \boldsymbol{\Sigma}^{-1/2} \) with matrix elements \( a_{ij} = x_{ij}/\sigma_i \) and the vector \( \boldsymbol{b} \) with elements \( b_i = y_i/\sigma_i \).
</div>
</div>


<p>
<!-- !split -->
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->

<p>
We can rewrite
$$
\frac{\partial \chi^2(\boldsymbol{\theta})}{\partial \boldsymbol{\theta}} = 0 = \boldsymbol{A}^T\left( \boldsymbol{b}-\boldsymbol{A}\boldsymbol{\theta}\right),  
$$

as
$$
\boldsymbol{A}^T\boldsymbol{b} = \boldsymbol{A}^T\boldsymbol{A}\boldsymbol{\theta},  
$$

and if the matrix \( \boldsymbol{A}^T\boldsymbol{A} \) is invertible we have the solution
$$
\boldsymbol{\theta} =\left(\boldsymbol{A}^T\boldsymbol{A}\right)^{-1}\boldsymbol{A}^T\boldsymbol{b}.
$$
</div>
</div>


<p>
<!-- !split -->
<div class="panel panel-default">
<div class="panel-body">
<p> <!-- subsequent paragraphs come in larger fonts, so start with a paragraph -->

<p>
If we then introduce the matrix
$$
\boldsymbol{H} =  \left(\boldsymbol{A}^T\boldsymbol{A}\right)^{-1},
$$

we have then the following expression for the parameters \( \theta_j \) (the matrix elements of \( \boldsymbol{H} \) are \( h_{ij} \))
$$
\theta_j = \sum_{k=0}^{p-1}h_{jk}\sum_{i=0}^{n-1}\frac{y_i}{\sigma_i}\frac{x_{ik}}{\sigma_i} = \sum_{k=0}^{p-1}h_{jk}\sum_{i=0}^{n-1}b_ia_{ik}
$$

We state without proof the expression for the uncertainty  in the parameters \( \theta_j \) as (we leave this as an exercise)
$$
\sigma^2(\theta_j) = \sum_{i=0}^{n-1}\sigma_i^2\left( \frac{\partial \theta_j}{\partial y_i}\right)^2, 
$$

resulting in 
$$
\sigma^2(\theta_j) = \left(\sum_{k=0}^{p-1}h_{jk}\sum_{i=0}^{n-1}a_{ik}\right)\left(\sum_{l=0}^{p-1}h_{jl}\sum_{m=0}^{n-1}a_{ml}\right) = h_{jj}!
$$
</div>
</div>


<!-- ------------------- end of main content --------------- -->

</div>  <!-- end container -->
<!-- include javascript, jQuery *first* -->
<script src="https://ajax.googleapis.com/ajax/libs/jquery/1.10.2/jquery.min.js"></script>
<script src="https://netdna.bootstrapcdn.com/bootstrap/3.0.0/js/bootstrap.min.js"></script>

<!-- Bootstrap footer
<footer>
<a href="http://..."><img width="250" align=right src="http://..."></a>
</footer>
-->


<center style="font-size:80%">
<!-- copyright --> &copy; 2018-2020, Christian Forss&#233;n. Released under CC Attribution-NonCommercial 4.0 license
</center>


</body>
</html>
    

